{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","collapsed_sections":["nerYYhdJLoY0"],"toc_visible":true,"machine_shape":"hm","mount_file_id":"1F_O6kqqgJxm1lue9XMqg8UiOovWnPUtF","authorship_tag":"ABX9TyNkqKfBeyvAtZSk5B4dXf37"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 前処理"],"metadata":{"id":"nerYYhdJLoY0"}},{"cell_type":"markdown","source":["## GPU info"],"metadata":{"id":"LXNVcK-YLxbT"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g2YtrWHnIRsH","executionInfo":{"status":"ok","timestamp":1739968753769,"user_tz":-540,"elapsed":224,"user":{"displayName":"opu就活用","userId":"12365573474540776425"}},"outputId":"42648a76-defd-4fd6-c88a-bef23ae56c9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Feb 19 12:39:30 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   58C    P0            109W /  400W |    2759MiB /  40960MiB |     64%      Default |\n","|                                         |                        |             Disabled |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","+-----------------------------------------------------------------------------------------+\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"]},{"cell_type":"markdown","source":["## Google Drive マウント"],"metadata":{"id":"yt3j2V6lL0i8"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"897syaAvL9lL","executionInfo":{"status":"ok","timestamp":1739967059908,"user_tz":-540,"elapsed":24682,"user":{"displayName":"opu就活用","userId":"12365573474540776425"}},"outputId":"94638732-8531-48f9-b987-326a92715889"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## module インストール (初回のみ)"],"metadata":{"id":"EoHV7JLwMKIS"}},{"cell_type":"code","source":["# ! pip install mecab-python3 unidic-lite #MeCab\n","# ! pip install fugashi\n","\n","# !cp -r /usr/local/lib/python3.10/dist-packages/fugashi /content/drive/MyDrive/colab-packages/\n","# !cp -r /usr/local/lib/python3.10/dist-packages/fugashi-1.3.2.dist-info /content/drive/MyDrive/colab-packages/\n","# !cp -r /usr/local/lib/python3.10/dist-packages/fugashi.libs /content/drive/MyDrive/colab-packages/\n","# !cp -r /usr/local/lib/python3.10/dist-packages/mecab_python3-1.0.9.dist-info /content/drive/MyDrive/colab-packages/\n","# !cp -r /usr/local/lib/python3.10/dist-packages/mecab_python3.libs /content/drive/MyDrive/colab-packages/\n","# !cp -r /usr/local/lib/python3.10/dist-packages/unidic_lite /content/drive/MyDrive/colab-packages/\n","# !cp -r /usr/local/lib/python3.10/dist-packages/unidic_lite-1.0.8.dist-info /content/drive/MyDrive/colab-packages/"],"metadata":{"id":"hJNeCPiUMK6H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# import"],"metadata":{"id":"mkMR0sNAOey9"}},{"cell_type":"code","source":["# install していた分\n","!cp -r /content/drive/MyDrive/colab-packages/fugashi /usr/local/lib/python3.10/dist-packages/fugashi\n","!cp -r /content/drive/MyDrive/colab-packages/fugashi-1.3.2.dist-info /usr/local/lib/python3.10/dist-packages/fugashi-1.3.2.dist-info\n","!cp -r /content/drive/MyDrive/colab-packages/fugashi.libs /usr/local/lib/python3.10/dist-packages/fugashi.libs\n","!cp -r /content/drive/MyDrive/colab-packages/mecab_python3-1.0.9.dist-info /usr/local/lib/python3.10/dist-packages/mecab_python3-1.0.9.dist-info\n","!cp -r /content/drive/MyDrive/colab-packages/mecab_python3.libs /usr/local/lib/python3.10/dist-packages/mecab_python3.libs\n","!cp -r /content/drive/MyDrive/colab-packages/unidic_lite /usr/local/lib/python3.10/dist-packages/unidic_lite\n","!cp -r /content/drive/MyDrive/colab-packages/unidic_lite-1.0.8.dist-info /usr/local/lib/python3.10/dist-packages/unidic_lite-1.0.8.dist-info"],"metadata":{"id":"MQBZvTuZbQt7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pathlib import Path\n","from datetime import datetime\n","import pytz\n","import copy\n","import os\n","import time\n","import numpy as np\n","import pandas as pd\n","from IPython.display import display\n","import matplotlib.pyplot as plt\n","import json\n","import random\n","from collections.abc import Iterable\n","from collections import defaultdict\n","import torch\n","import torch.nn as nn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n","from torch.utils.data import DataLoader, Dataset\n","from torch import FloatTensor, LongTensor\n","from tqdm import tqdm, trange\n","from transformers import (\n","    AutoModelForSequenceClassification,\n","    AutoModel,\n","    AutoTokenizer,\n","    PreTrainedModel,\n",")\n","from transformers.modeling_outputs import SequenceClassifierOutput,BaseModelOutputWithPast\n","from transformers.optimization import get_linear_schedule_with_warmup\n","from transformers.tokenization_utils import BatchEncoding, PreTrainedTokenizer\n","\n","import sys\n","sys.path.append('/content/drive/MyDrive/ex2024/')\n","\n","# colab 関連\n","from google.colab import files\n","\n","# 乱数シードの設定\n","def set_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n"],"metadata":{"id":"mQhRa_ftOhhA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#utility 関数"],"metadata":{"id":"E6w-3-5bP_VC"}},{"cell_type":"code","source":["def load_jsonl(path: Path | str) -> pd.DataFrame:\n","    path = Path(path)\n","    return pd.read_json(path, lines=True)\n","\n","\n","def load_json(path: Path | str) -> dict:\n","    path = Path(path)\n","    with path.open() as f:\n","        data = json.load(f)\n","    return data\n","\n","def make_dir(path):\n","    os.makedirs(path, exist_ok=True)"],"metadata":{"id":"mdlLNjUnP-xF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Args (実験に使う定数などなど〜)"],"metadata":{"id":"lPF9l6WpwALE"}},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","class Args():\n","    def __init__(self):\n","        ###############################################################################\n","        # 現在の日時を取得してフォルダ名に使用\n","        self.current_time = datetime.now(pytz.utc).astimezone(pytz.timezone('Asia/Tokyo')).strftime('%Y-%m-%d_%H-%M-%S')\n","\n","        # 実験結果を保存するベースディレクトリ\n","        self.base_dir = Path('/content/drive/MyDrive/ex2024/results/')\n","\n","        # フォルダ名に日時を追加してパスを生成\n","        self.experiment_dir = self.base_dir / f'experiment_{self.current_time}'\n","\n","        # フォルダの作成 (必要であれば親ディレクトリも作成)\n","        self.experiment_dir.mkdir(parents=True, exist_ok=True)\n","        print(f\"Experiment directory created at: {self.experiment_dir}\")\n","\n","        #訓練後モデル保存先\n","        self.output_model_dir: Path = self.experiment_dir / f'model.bin'\n","\n","        #学習結果保存先\n","        self.result_csv_dir = {\n","            'train': self.experiment_dir / f'train_result.csv',\n","            'val': self.experiment_dir / f'val_result.csv',\n","            'test': self.experiment_dir / f'test_result.csv'\n","        }\n","        self.result_confusion_matrix_dir = {\n","            'train': self.experiment_dir / f'train_cm.json',\n","            'val': self.experiment_dir / f'val_cm.json',\n","            'test': self.experiment_dir / f'test_cm.json'\n","        }\n","        #学習曲線保存先\n","        self.result_accuracy_dir = self.experiment_dir / f'train_val_accuracy.png'\n","        self.result_loss_dir = self.experiment_dir / f'train_val_loss.png'\n","        self.result_param_dir = self.experiment_dir / f'train_param.png'\n","        ###############################################################################\n","        #BERT訓練済みモデル\n","        self.bert_pretrained_model_name = \"cl-tohoku/bert-base-japanese-v3\"\n","        self.max_seq_len = 512 #BERT入力列最大値\n","\n","        #livedoorニュースコーパスデータセットへのパス\n","        self.dataset_dir: Path = Path('/content/drive/MyDrive/ex2024/livedoor/datasets/livedoor/')\n","        self.summary_dir: Path = Path('/content/drive/MyDrive/ex2024/livedoor/datasets/livedoor/summary/')\n","\n","        #---学習周り---\n","        self.batch_size: int = 16\n","        self.epochs: int = 20\n","        self.lr: float = 3e-5\n","        self.n_class: int = 9\n","        self.seed: int = 42\n","        self.trial: int = 5\n","\n","        self.sampling_flag = {'train': False, 'val': False, 'test': False}\n","        self.sampling_rate = {'train': 1.0, 'val': 1.0, 'test': 1.0}"],"metadata":{"id":"OODC4D0pwBa4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Net"],"metadata":{"id":"w_VP0y_Q8ABM"}},{"cell_type":"code","source":["class BertPretrainedNet(nn.Module):\n","    def __init__(self, _model_name='cl-tohoku/bert-base-japanese-v3', _fine_tuning_last=True, _fine_tuning_all=False):\n","        super().__init__()\n","        self.bert = AutoModel.from_pretrained(_model_name)\n","        self.hidden_size: int = self.bert.config.hidden_size #768\n","        self.fine_tuning_last = _fine_tuning_last\n","        self.fine_tuning_all = _fine_tuning_all\n","\n","        if self.fine_tuning_all is False:\n","            # Bertの1〜11段目は更新せず、12段目とSequenceClassificationのLayerのみトレーニングする。\n","            # 一旦全部のパラメータのrequires_gradをFalseで更新\n","            for name, param in self.bert.named_parameters():\n","                param.requires_grad = False\n","            if self.fine_tuning_last:\n","                # Bert encoderの最終レイヤのrequires_gradをTrueで更新\n","                for name, param in self.bert.encoder.layer[-1].named_parameters():\n","                    param.requires_grad = True\n","\n","    def forward(self, x_input_ids, x_attention_mask):\n","        x = self.bert(x_input_ids, x_attention_mask)\n","        return x\n","\n","class SimpleClassifierNet(nn.Module):\n","    def __init__(self, _in_dim=768, _out_dim=9):\n","        super().__init__()\n","        self.in_dim = _in_dim\n","        self.out_dim = _out_dim\n","        self.classifier = nn.Linear(self.in_dim, self.out_dim, bias=True)\n","\n","    def forward(self, x):\n","        return self.classifier(x)\n","\n","class Yamato_Pooling(nn.Module):\n","    def __init__(self, init_weights=[nn.Parameter(torch.tensor(0.5)), nn.Parameter(torch.tensor(0.5))]):\n","        super().__init__()\n","        self.p = init_weights[0]\n","        self.q = init_weights[1]\n","\n","    def get_param(self):\n","        # pとqを計算（q = 1 - p)\n","        new_p = self.p.detach()\n","        new_q = self.q.detach()\n","        sum = (new_p ** 2 + new_q ** 2)\n","        p = new_p ** 2 / sum\n","        q = new_q ** 2 / sum\n","        return {'p':p.cpu().numpy(),'q':q.cpu().numpy()}\n","\n","    def forward(self, x_cls, x_avg):\n","        # pとqを計算（q = 1 - p）\n","        sum = (self.p ** 2 + self.q ** 2)\n","        p = self.p ** 2 / sum\n","        q = self.q ** 2 / sum\n","        return q * x_cls + p * x_avg\n","\n","class Takayama_Pooling(nn.Module):\n","    def __init__(self, init_weights=[nn.Parameter(torch.tensor(1/3.0)), nn.Parameter(torch.tensor(1/3.0)), nn.Parameter(torch.tensor(1/3.0))]):\n","        super().__init__()\n","        self.p = init_weights[0]\n","        self.q = init_weights[1]\n","        self.r = init_weights[2]\n","\n","    def get_param(self):\n","        # pとqを計算（q = 1 - p)\n","        new_p = self.p.detach()\n","        new_q = self.q.detach()\n","        new_r = self.r.detach()\n","        sum = (new_p ** 2 + new_q ** 2 + new_r ** 2)\n","        p = new_p ** 2 / sum\n","        q = new_q ** 2 / sum\n","        r = new_r ** 2 / sum\n","        return {'p':p.cpu().numpy(),'q':q.cpu().numpy(), 'r':r.cpu().numpy()}\n","\n","    def forward(self, x_cls, x_avg, s_cls):\n","        # pとqを計算（q = 1 - p）\n","        sum = (self.p ** 2 + self.q ** 2 + self.r ** 2)\n","        p = self.p ** 2 / sum\n","        q = self.q ** 2 / sum\n","        r = self.r ** 2 / sum\n","        return p * x_cls + q * x_avg + r * s_cls"],"metadata":{"id":"gHkHGHjg8B1C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Takayama_Pooling_pqrs(nn.Module):\n","    def __init__(self, init_weights=[nn.Parameter(torch.tensor(1/4.0)), nn.Parameter(torch.tensor(1/4.0)), nn.Parameter(torch.tensor(1/4.0)), nn.Parameter(torch.tensor(1/4.0))]):\n","        super().__init__()\n","        self.p = init_weights[0]\n","        self.q = init_weights[1]\n","        self.r = init_weights[2]\n","        self.s = init_weights[3]\n","\n","    def get_param(self):\n","        # pとqを計算（q = 1 - p)\n","        new_p = self.p.detach()\n","        new_q = self.q.detach()\n","        new_r = self.r.detach()\n","        new_s = self.s.detach()\n","        sum = (new_p ** 2 + new_q ** 2 + new_r ** 2 + new_s ** 2)\n","        p = new_p ** 2 / sum\n","        q = new_q ** 2 / sum\n","        r = new_r ** 2 / sum\n","        s = new_s ** 2 / sum\n","        return {'p':p.cpu().numpy(),'q':q.cpu().numpy(), 'r':r.cpu().numpy(), 's':s.cpu().numpy()}\n","\n","    def forward(self, x_cls, x_avg, s_cls, s_avg):\n","        # pとqを計算（q = 1 - p）\n","        sum = (self.p ** 2 + self.q ** 2 + self.r ** 2 + self.s ** 2)\n","        p = self.p ** 2 / sum\n","        q = self.q ** 2 / sum\n","        r = self.r ** 2 / sum\n","        s = self.s ** 2 / sum\n","        return p * x_cls + q * x_avg + r * s_cls + s * s_avg"],"metadata":{"id":"KN1hG5tHWiFZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ExperimentNet_2024_0812(nn.Module):\n","    def __init__(self, _args: Args):\n","        super().__init__()\n","        self.bert = BertPretrainedNet(_model_name=_args.bert_pretrained_model_name, _fine_tuning_last=True, _fine_tuning_all=False)\n","        self.fc = SimpleClassifierNet()\n","    def forward(self, x):\n","        bert_out = self.bert(x)[0] # 最後の隠れ層\n","        out = self.fc(bert_out[:, 0, :]) # [CLS] に相当する部分のみ使う\n","        return out\n","\n","class ExperimentNet_2024_poster_ex1(nn.Module):\n","    def __init__(self, _args: Args):\n","        super().__init__()\n","        self.bert = BertPretrainedNet(_model_name=_args.bert_pretrained_model_name, _fine_tuning_last=True, _fine_tuning_all=False)\n","        self.pooling = Yamato_Pooling()\n","        self.classifier = SimpleClassifierNet()\n","    def forward(self, x_input_ids, x_attention_mask):\n","        bert_out = self.bert(x_input_ids, x_attention_mask)[0]\n","         # [CLS]トークンのベクトルを取得\n","        cls_vec = bert_out[:, 0, :]  # [batch_size, hidden_size]\n","        # 残りのトークンのベクトルを平均プーリング\n","        avg_vec = bert_out[:, 1:, :].mean(dim=1)  # [batch_size, hidden_size]\n","\n","        # 重み付き和を計算\n","        weighted_sum = self.pooling(cls_vec, avg_vec)  # [batch_size, hidden_size]\n","\n","        # 分類器に渡す\n","        return self.classifier(weighted_sum)  # [batch_size, num_classes]\n","\n","class ExperimentNet_2024_poster_ex2(nn.Module):\n","    def __init__(self, _args: Args):\n","        super().__init__()\n","        self.bert = BertPretrainedNet(_model_name=_args.bert_pretrained_model_name, _fine_tuning_last=True, _fine_tuning_all=False)\n","        self.bert_summary = BertPretrainedNet(_model_name=_args.bert_pretrained_model_name, _fine_tuning_last=True, _fine_tuning_all=False)\n","        self.pooling = Takayama_Pooling()\n","        self.classifier = SimpleClassifierNet()\n","    def forward(self, x_input_ids, x_attention_mask, s_input_ids, s_attention_mask):\n","        bert_out = self.bert(x_input_ids, x_attention_mask)[0]\n","         # [CLS]トークンのベクトルを取得\n","        cls_vec = bert_out[:, 0, :]  # [batch_size, hidden_size]\n","        # 残りのトークンのベクトルを平均プーリング\n","        out_without_cls = bert_out[:, 1:, :]\n","        expanded_mask = x_attention_mask[:, 1:].unsqueeze(-1).expand(out_without_cls.size())\n","        masked_embeddings = out_without_cls * expanded_mask\n","        # 各バッチのトークン数（パディング部分を除く）を計算\n","        sum_mask = expanded_mask.sum(dim=1)\n","        sum_mask = torch.clamp(sum_mask, min=1e-9)  # 0除算を防ぐためにクランプ\n","        avg_vec = masked_embeddings.sum(dim=1) / sum_mask\n","        #summaryのcls\n","        summary_cls_vec = self.bert_summary(s_input_ids, s_attention_mask)[0][:, 0, :]\n","        # 重み付き和を計算\n","        weighted_sum = self.pooling(cls_vec, avg_vec, summary_cls_vec)  # [batch_size, hidden_size]\n","        # 分類器に渡す\n","        return self.classifier(weighted_sum)  # [batch_size, num_classes]"],"metadata":{"id":"ZQE-LNEkqPn_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ExperimentNet_2024_poster_ex2_pqrs(nn.Module):\n","    def __init__(self, _args: Args):\n","        super().__init__()\n","        self.bert = BertPretrainedNet(_model_name=_args.bert_pretrained_model_name, _fine_tuning_last=True, _fine_tuning_all=False)\n","        self.bert_summary = BertPretrainedNet(_model_name=_args.bert_pretrained_model_name, _fine_tuning_last=True, _fine_tuning_all=False)\n","        self.pooling = Takayama_Pooling_pqrs()\n","        self.classifier = SimpleClassifierNet()\n","    def forward(self, x_input_ids, x_attention_mask, s_input_ids, s_attention_mask):\n","        bert_out = self.bert(x_input_ids, x_attention_mask)[0]\n","         # [CLS]トークンのベクトルを取得\n","        cls_vec = bert_out[:, 0, :]  # [batch_size, hidden_size]\n","        # 残りのトークンのベクトルを平均プーリング\n","        out_without_cls = bert_out[:, 1:, :]\n","        expanded_mask = x_attention_mask[:, 1:].unsqueeze(-1).expand(out_without_cls.size())\n","        masked_embeddings = out_without_cls * expanded_mask\n","        # 各バッチのトークン数（パディング部分を除く）を計算\n","        sum_mask = expanded_mask.sum(dim=1)\n","        sum_mask = torch.clamp(sum_mask, min=1e-9)  # 0除算を防ぐためにクランプ\n","        avg_vec = masked_embeddings.sum(dim=1) / sum_mask\n","\n","        summary_bert_out = self.bert_summary(s_input_ids, s_attention_mask)[0]\n","        #summaryのcls\n","        summary_cls_vec = summary_bert_out[:, 0, :]\n","        # 残りのトークンのベクトルを平均プーリング\n","        out_without_cls = summary_bert_out[:, 1:, :]\n","        expanded_mask = s_attention_mask[:, 1:].unsqueeze(-1).expand(out_without_cls.size())\n","        masked_embeddings = out_without_cls * expanded_mask\n","        # 各バッチのトークン数（パディング部分を除く）を計算\n","        sum_mask = expanded_mask.sum(dim=1)\n","        sum_mask = torch.clamp(sum_mask, min=1e-9)  # 0除算を防ぐためにクランプ\n","        summary_avg_vec = masked_embeddings.sum(dim=1) / sum_mask\n","\n","        # 重み付き和を計算\n","        weighted_sum = self.pooling(cls_vec, avg_vec, summary_cls_vec, summary_avg_vec)  # [batch_size, hidden_size]\n","        # 分類器に渡す\n","        return self.classifier(weighted_sum)  # [batch_size, num_classes]"],"metadata":{"id":"HgxghcUSXLAf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Experiment"],"metadata":{"id":"e2TVGotJ2L9v"}},{"cell_type":"code","source":["class Experiment():\n","    def __init__(self, _args: Args):\n","        #実験設定\n","        self.args = _args\n","        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","        self.phase = ['train', 'val', 'test']\n","\n","        #トークナイザー設定\n","        self.tokenizer = AutoTokenizer.from_pretrained(\n","            self.args.bert_pretrained_model_name,\n","            model_max_length = self.args.max_seq_len,\n","        )\n","        #モデル(Net)\n","        self.net = ExperimentNet_2024_poster_ex2_pqrs(_args=self.args).eval().to(self.device)\n","        #データローダー\n","        self.dataloader = defaultdict(lambda: defaultdict(list))\n","        #データセット(DataFrame)\n","        self.dataset = defaultdict(lambda: defaultdict(list))\n","        for p in self.phase:\n","            self.dataloader[p] = self.load_dataset(phase = p, shuffle = (p=='train'), sampling_flag = self.args.sampling_flag[p], sampling_rate = self.args.sampling_rate[p])\n","        #損失関数とオプティマイザーの定義\n","        self.criterion = torch.nn.CrossEntropyLoss()\n","        # self.optimizer = torch.optim.Adam(self.net.parameters(), lr=self.args.lr)\n","        self.optimizer = torch.optim.Adam([\n","            {'params':self.net.bert.parameters(), 'lr':self.args.lr},\n","            {'params':self.net.bert_summary.parameters(), 'lr':self.args.lr},\n","            {'params':self.net.pooling.p, 'lr':self.args.lr},\n","            {'params':self.net.pooling.q, 'lr':self.args.lr},\n","            {'params':self.net.pooling.r, 'lr':self.args.lr},\n","            {'params':self.net.pooling.s, 'lr':self.args.lr},\n","            {'params':self.net.classifier.parameters(), 'lr':self.args.lr}\n","        ])\n","        #結果保存用\n","        self.results = defaultdict(lambda: defaultdict(list))\n","        self.results['train'] = []\n","        self.results['val'] = []\n","        self.results['test'] = []\n","        self.results['param'] = []\n","        self.best_model_state = {\n","            'epoch': 0,\n","            'accuracy': 0.0 - 1e-5,\n","            'best_model_state_dict': None\n","        }\n","\n","    def results_update(self, phase, new_eval_metrics):\n","        self.results[phase].append(new_eval_metrics)\n","        if phase == 'val':\n","            if self.best_model_state['accuracy'] < new_eval_metrics['accuracy']:\n","                print('\\nbest score updated :{0}'.format(new_eval_metrics['accuracy']))\n","                self.best_model_state = {\n","                    'epoch': new_eval_metrics['epoch'],\n","                    'accuracy': new_eval_metrics['accuracy'],\n","                    'best_model_state_dict': self.net.state_dict().copy()\n","                }\n","\n","    def load_dataset(self, phase, shuffle: bool = False, sampling_flag = False, sampling_rate = 1.0):\n","        path = self.args.dataset_dir / f'{phase}.jsonl'\n","        summary_data = load_json(self.args.summary_dir / f'livedoor_summary_plamo_beta_{phase}.json')\n","        original = load_jsonl(path)\n","        #summary列を追加\n","        original['summary'] = original['category-id'].map(summary_data)\n","        if sampling_flag == True:\n","            original = original.groupby('label', group_keys=False).apply(\n","                lambda x: x.sample(frac=sampling_rate, random_state=self.args.seed)\n","            ).reset_index(drop=True)\n","        data = original.to_dict(orient=\"records\")\n","        self.dataset[phase] = pd.DataFrame(data.copy())\n","        return DataLoader(data, collate_fn=self.collate_fn, batch_size=self.args.batch_size,shuffle=shuffle,num_workers=2,pin_memory=True)\n","\n","    # カスタムcollate_fnの定義\n","    def collate_fn(self, batch):\n","        texts = [data['title'] + \" [SEP] \" + data['body'] for data in batch]\n","        labels = torch.tensor([data['label'] for data in batch])\n","        encoding = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n","        encoding['labels'] = labels\n","        #category-idから元のデータを参照できるようにする\n","        category_ids = [data['category-id'] for data in batch]\n","        #summary\n","        summary_texts = [data['summary'] for data in batch]\n","        summary_encoding = self.tokenizer(summary_texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n","        return {'input_ids': encoding['input_ids'], 'attention_mask': encoding['attention_mask'], 'labels': labels, 'category-ids': category_ids, 'summary_input_ids': summary_encoding['input_ids'], 'summary_attention_mask': summary_encoding['attention_mask']}\n","\n","    def category_id2article(self, phase, category_id):\n","        return self.dataset[phase][self.dataset[phase]['category-id'] == category_id]\n","\n","\n","    def train(self):\n","        self.results_update('param', self.net.pooling.get_param())\n","        for epoch in range(self.args.epochs):\n","            time_start = time.time()\n","            print('Epoch {}/{}'.format(epoch + 1, self.args.epochs))\n","            print('-------------')\n","            print('p:{0}, q:{1}, r:{2}, s:{3}\\n'.format(self.results['param'][-1]['p'], self.results['param'][-1]['q'], self.results['param'][-1]['r'], self.results['param'][-1]['s']))\n","\n","            for phase in ['train', 'val']:\n","                self.net.train() if phase == 'train' else self.net.eval()\n","                if phase == 'val':\n","                    self.results_update('param', self.net.pooling.get_param())\n","                dataloader_size = len(self.dataloader[phase])\n","                #### epochごとの記録保存用\n","                running_loss = 0.0\n","                all_preds = []\n","                all_labels = []\n","                ####\n","\n","                for batch_idx, batch in enumerate(self.dataloader[phase]):\n","                    print(\"{}/{}\".format(batch_idx+1,dataloader_size))\n","\n","                    ####入力データの情報\n","                    input_ids = batch['input_ids'].to(self.device)\n","                    attention_mask = batch['attention_mask'].to(self.device)\n","                    labels = batch['labels'].to(self.device)\n","                    category_ids = batch['category-ids']\n","\n","                    s_input_ids = batch['summary_input_ids'].to(self.device)\n","                    s_attention_mask = batch['summary_attention_mask'].to(self.device)\n","                    ####\n","\n","                    self.optimizer.zero_grad()\n","                    with torch.set_grad_enabled(phase == 'train'):\n","                        y_pred = self.net(input_ids, attention_mask, s_input_ids, s_attention_mask)\n","                        _, predicted = torch.max(y_pred.data, 1)\n","                        # loss 計算・加算\n","                        loss = self.criterion(y_pred, labels)\n","\n","                        running_loss += loss.item()\n","                        all_preds.extend(predicted.cpu().numpy())\n","                        all_labels.extend(labels.cpu().numpy())\n","\n","                        #### 訓練時のみバックプロパゲーション\n","                        if phase == 'train':\n","                            loss.backward()\n","                            self.optimizer.step()\n","                            #### ここでパラメーターp,qの値を取って来ると良い\n","\n","                mean_loss = running_loss / dataloader_size\n","                accuracy = accuracy_score(all_labels, all_preds)\n","                precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n","                cm = confusion_matrix(all_labels, all_preds)\n","                ##################\n","                new_eval_metrics = {\n","                    'accuracy': accuracy,\n","                    'recall': recall,\n","                    'precision': precision,\n","                    'f1': f1,\n","                    'loss': mean_loss,\n","                    'confusion_matrix': cm,\n","                    'epoch': epoch\n","                }\n","                ##################\n","                self.results_update(phase, new_eval_metrics)\n","                print(new_eval_metrics['confusion_matrix'])\n","\n","                # Validation 結果\n","                if phase == 'val':\n","                    print(\"---Validation---\")\n","                else:\n","                    print(\"---TRAIN---\")\n","                print(\"Acc : %.4f\" % accuracy)\n","                print(\"loss : {}\".format(mean_loss))\n","\n","            time_finish = time.time() - time_start\n","            print(\"====================================\")\n","            print(\"残り時間 : {0}\".format(time_finish * (self.args.epochs - epoch)))\n","\n","        self.save_model()\n","        self.visualize()\n","        return 0\n","\n","    def save_model(self):\n","        torch.save(self.best_model_state['best_model_state_dict'], self.args.output_model_dir)\n","\n","    def save_log(self):\n","        for phase in self.phase:\n","            #混合行列以外の acc や loss の推移を dataframe にして csv 保存\n","            filtered_data = [{k: v for k, v in d.items() if k != 'confusion_matrix'} for d in self.results[phase]]\n","            df = pd.DataFrame(filtered_data)\n","            df.to_csv(self.args.result_csv_dir[phase], index=False)\n","            if phase == 'test':\n","                display(df)\n","            #混合行列をjsonに保存\n","            cm_list = [d['confusion_matrix'].tolist() for d in self.results[phase]]\n","            with open(self.args.result_confusion_matrix_dir[phase], 'w') as f:\n","                json.dump(cm_list, f)\n","\n","\n","    def visualize(self):\n","        #学習曲線\n","        epochs = list(range(1, self.args.epochs + 1))  # 1からepochsまでのエポック\n","        train_acc = [d['accuracy'] for d in self.results['train']]\n","        val_acc = [d['accuracy'] for d in self.results['val']]\n","        train_loss = [d['loss'] for d in self.results['train']]\n","        val_loss = [d['loss'] for d in self.results['val']]\n","        p_data = [d['p'] for d in self.results['param']]\n","        q_data = [d['q'] for d in self.results['param']]\n","        r_data = [d['r'] for d in self.results['param']]\n","        s_data = [d['s'] for d in self.results['param']]\n","\n","        self.make_plot(epochs, [{'data':train_acc, 'label':'Train Accuracy'}, {'data':val_acc, 'label':'Validation Accuracy'}],\n","                                title='Train and Validation Accuracy',\n","                                xlabel='Epoch', ylabel='Accuracy', path=self.args.result_accuracy_dir)\n","        self.make_plot(epochs, [{'data':train_loss, 'label':'Train Loss'}, {'data':val_loss, 'label':'Validation Loss'}],\n","                                title='Train and Validation Loss',\n","                                xlabel='Epoch', ylabel='Loss', path=self.args.result_loss_dir)\n","        self.make_plot(list(range(0, self.args.epochs + 1)), [{'data':p_data, 'label':'p'}, {'data':q_data, 'label':'q'}, {'data':r_data, 'label':'r'}, {'data':s_data, 'label':'s'}],\n","                                title='p, q, r, s',\n","                                xlabel='Epoch', ylabel='Value', path=self.args.result_param_dir)\n","        #paramの変位をグラフ化\n","        pass\n","\n","\n","    def make_plot(self, x_data, y_data, title, xlabel, ylabel, path):\n","        # グラフの作成\n","        plt.clf()\n","        plt.figure(figsize=(10, 6))\n","        for d in y_data:\n","            plt.plot(x_data, d['data'], label=d['label'])\n","        # グラフのタイトルとラベル\n","        plt.title(title)\n","        plt.xlabel(xlabel)\n","        plt.ylabel(ylabel)\n","        # 凡例の表示\n","        plt.legend()\n","        # グラフをPNGファイルとして保存\n","        plt.savefig(path)\n","        # グラフの表示\n","        plt.show()\n","\n","\n","    def test(self):\n","        #ベストモデルをテストデータにロードして評価\n","        self.net.load_state_dict(self.best_model_state['best_model_state_dict'])\n","        self.net.eval()\n","\n","        all_preds = []\n","        all_labels = []\n","\n","        with torch.no_grad():\n","            for batch_idx, batch in enumerate(self.dataloader['test']):\n","                ####入力データの情報\n","                input_ids = batch['input_ids'].to(self.device)\n","                attention_mask = batch['attention_mask'].to(self.device)\n","                labels = batch['labels'].to(self.device)\n","                category_ids = batch['category-ids']\n","                s_input_ids = batch['summary_input_ids'].to(self.device)\n","                s_attention_mask = batch['summary_attention_mask'].to(self.device)\n","                ####\n","                y_pred = self.net(input_ids, attention_mask, s_input_ids, s_attention_mask)\n","                _, predicted = torch.max(y_pred.data, 1)\n","                all_preds.extend(predicted.cpu().numpy())\n","                all_labels.extend(labels.cpu().numpy())\n","\n","                pre = predicted.cpu().numpy()\n","                labs = labels.cpu().numpy()\n","                for idx in range(0, pre.shape[0]):\n","                    if pre[idx] != labs[idx]:\n","                        print(category_ids[idx])\n","\n","        # テストデータの評価指標の計算\n","        accuracy = accuracy_score(all_labels, all_preds)\n","        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n","        cm = confusion_matrix(all_labels, all_preds)\n","        ##################\n","        new_eval_metrics = {\n","            'accuracy': accuracy,\n","            'recall': recall,\n","            'precision': precision,\n","            'f1': f1,\n","            'loss': None,\n","            'confusion_matrix': cm,\n","            'epoch': None\n","        }\n","        ##################\n","        self.results_update('test', new_eval_metrics)\n","        print(new_eval_metrics['confusion_matrix'])\n","\n","        self.save_log()\n","\n","    def run(self):\n","        print(\"ex start\")\n","        print(self.device)\n","        self.label_distribution()\n","        self.train()\n","        self.test()\n","        # display(self.dataset['train'])\n","\n","    def label_distribution(self):\n","        # ラベルごとのデータ数を集計\n","        for p in self.phase:\n","            label_counts = self.dataset[p]['label'].value_counts()\n","            print(p, label_counts)\n","\n","    def load_confusion_matrix(self, path):\n","        # JSONファイルからリストを読み込む\n","        with open(path, 'r') as f:\n","            loaded_matrices_list = json.load(f)\n","        # リストをNumPy配列に変換\n","        loaded_matrices = [np.array(matrix) for matrix in loaded_matrices_list]\n","        return loaded_matrices"],"metadata":{"id":"7ue0lP8-2NbS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install fugashi\n","!pip install unidic_lite"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ojr1JtyJZ1pZ","executionInfo":{"status":"ok","timestamp":1739967158140,"user_tz":-540,"elapsed":18705,"user":{"displayName":"opu就活用","userId":"12365573474540776425"}},"outputId":"2a68f670-c315-47f8-8c40-fa3796615025"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting fugashi\n","  Downloading fugashi-1.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n","Downloading fugashi-1.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (698 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/698.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.0/698.0 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: fugashi\n","Successfully installed fugashi-1.4.0\n","Collecting unidic_lite\n","  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: unidic_lite\n","  Building wheel for unidic_lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for unidic_lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658818 sha256=aaa3662a2fee7643c8fa02e98d267168b5f10707134b0ffe2c34590c5eebef65\n","  Stored in directory: /root/.cache/pip/wheels/b7/fd/e9/ea4459b868e6d2902e8d80e82dbacb6203e05b3b3a58c64966\n","Successfully built unidic_lite\n","Installing collected packages: unidic_lite\n","Successfully installed unidic_lite-1.0.8\n"]}]},{"cell_type":"markdown","source":["# Main"],"metadata":{"id":"TQajAeCk2-H1"}},{"cell_type":"code","source":["ex = Experiment(_args = Args())\n","ex.run()\n","print('---finish---')"],"metadata":{"id":"QnD6oRvb2_N8"},"execution_count":null,"outputs":[]}]}