class weight : tensor([0.7557, 0.1616])
best:lr 3.8508699932757367e-07
EPOCH : 1 / 50
VAL_LOSS : 0.6882354915142059 
VAL_ACCURACY : 0.26658624849215923
VAL_F1 : 0.38336713964877245

EPOCH : 2 / 50
VAL_LOSS : 0.6519698512095672 
VAL_ACCURACY : 0.2750301568154403
VAL_F1 : 0.37977296150151496

EPOCH : 3 / 50
VAL_LOSS : 0.6369769968665563 
VAL_ACCURACY : 0.29674306393244876
VAL_F1 : 0.3843716997852921

EPOCH : 4 / 50
VAL_LOSS : 0.6164803848816798 
VAL_ACCURACY : 0.3558504221954162
VAL_F1 : 0.40268456342405745

EPOCH : 5 / 50
VAL_LOSS : 0.5994555399968073 
VAL_ACCURACY : 0.4077201447527141
VAL_F1 : 0.4203069654138211

EPOCH : 6 / 50
VAL_LOSS : 0.5826543208498222 
VAL_ACCURACY : 0.5042219541616405
VAL_F1 : 0.4613368279354003

EPOCH : 7 / 50
VAL_LOSS : 0.5701601161406591 
VAL_ACCURACY : 0.5307599517490953
VAL_F1 : 0.4778523486133417

EPOCH : 8 / 50
VAL_LOSS : 0.558412736425033 
VAL_ACCURACY : 0.5560916767189384
VAL_F1 : 0.4917127067951566

EPOCH : 9 / 50
VAL_LOSS : 0.5414457447253741 
VAL_ACCURACY : 0.5838359469240049
VAL_F1 : 0.5064377678443392

EPOCH : 10 / 50
VAL_LOSS : 0.5260591306365453 
VAL_ACCURACY : 0.6272617611580217
VAL_F1 : 0.5339366511744915

EPOCH : 11 / 50
VAL_LOSS : 0.5108057936796775 
VAL_ACCURACY : 0.7249698431845597
VAL_F1 : 0.5985915488496144

EPOCH : 12 / 50
VAL_LOSS : 0.5017728736767402 
VAL_ACCURACY : 0.715319662243667
VAL_F1 : 0.5972696241343346

EPOCH : 13 / 50
VAL_LOSS : 0.48625306211985075 
VAL_ACCURACY : 0.7406513872135102
VAL_F1 : 0.6153846149348344

EPOCH : 14 / 50
VAL_LOSS : 0.47783837066246915 
VAL_ACCURACY : 0.758745476477684
VAL_F1 : 0.6323529407207031

EPOCH : 15 / 50
VAL_LOSS : 0.4611038261881241 
VAL_ACCURACY : 0.7804583835946924
VAL_F1 : 0.6499999995347855

EPOCH : 16 / 50
VAL_LOSS : 0.4510481128325829 
VAL_ACCURACY : 0.7973462002412546
VAL_F1 : 0.6705882348249828

EPOCH : 17 / 50
VAL_LOSS : 0.44042581892930543 
VAL_ACCURACY : 0.7937273823884198
VAL_F1 : 0.670520230748193

EPOCH : 18 / 50
VAL_LOSS : 0.4338198189552014 
VAL_ACCURACY : 0.8069963811821471
VAL_F1 : 0.6850393696087871

EPOCH : 19 / 50
VAL_LOSS : 0.4219580986178838 
VAL_ACCURACY : 0.8431845597104946
VAL_F1 : 0.7280334723221845

EPOCH : 20 / 50
VAL_LOSS : 0.42213518287126833 
VAL_ACCURACY : 0.8250904704463209
VAL_F1 : 0.7070707065957842

EPOCH : 21 / 50
VAL_LOSS : 0.4057656687039595 
VAL_ACCURACY : 0.8528347406513872
VAL_F1 : 0.740425531430901

EPOCH : 22 / 50
VAL_LOSS : 0.39894142689613193 
VAL_ACCURACY : 0.856453558504222
VAL_F1 : 0.7462686562320594

EPOCH : 23 / 50
VAL_LOSS : 0.38871479149048144 
VAL_ACCURACY : 0.8600723763570567
VAL_F1 : 0.7510729608879976

EPOCH : 24 / 50
VAL_LOSS : 0.3864653803981267 
VAL_ACCURACY : 0.8661037394451147
VAL_F1 : 0.7592190884500073

EPOCH : 25 / 50
VAL_LOSS : 0.3806118953686494 
VAL_ACCURACY : 0.8685162846803377
VAL_F1 : 0.7635574832439148

EPOCH : 26 / 50
VAL_LOSS : 0.3684414309950975 
VAL_ACCURACY : 0.8745476477683957
VAL_F1 : 0.770925109642774

EPOCH : 27 / 50
VAL_LOSS : 0.36138284750855887 
VAL_ACCURACY : 0.8841978287092883
VAL_F1 : 0.7857142852229851

EPOCH : 28 / 50
VAL_LOSS : 0.3581450521372832 
VAL_ACCURACY : 0.8745476477683957
VAL_F1 : 0.7719298240726474

EPOCH : 29 / 50
VAL_LOSS : 0.3492111446192631 
VAL_ACCURACY : 0.8769601930036188
VAL_F1 : 0.7753303959863669

EPOCH : 30 / 50
VAL_LOSS : 0.34730739003190625 
VAL_ACCURACY : 0.8757539203860072
VAL_F1 : 0.7746170673452496

EPOCH : 31 / 50
VAL_LOSS : 0.33997574333961195 
VAL_ACCURACY : 0.8841978287092883
VAL_F1 : 0.7866666661759704

EPOCH : 32 / 50
VAL_LOSS : 0.32954662952285546 
VAL_ACCURACY : 0.8926417370325693
VAL_F1 : 0.7990970649699107

EPOCH : 33 / 50
VAL_LOSS : 0.3276035049213813 
VAL_ACCURACY : 0.8902291917973462
VAL_F1 : 0.7955056174852873

EPOCH : 34 / 50
VAL_LOSS : 0.32213210658385205 
VAL_ACCURACY : 0.8914354644149578
VAL_F1 : 0.7972972968047541

EPOCH : 35 / 50
VAL_LOSS : 0.31694919300767094 
VAL_ACCURACY : 0.8986731001206273
VAL_F1 : 0.808219177587884

EPOCH : 36 / 50
VAL_LOSS : 0.3075114010045162 
VAL_ACCURACY : 0.8974668275030157
VAL_F1 : 0.8072562353342075

EPOCH : 37 / 50
VAL_LOSS : 0.30365333104362857 
VAL_ACCURACY : 0.9010856453558505
VAL_F1 : 0.8127853876335253

EPOCH : 38 / 50
VAL_LOSS : 0.2977047539674319 
VAL_ACCURACY : 0.9047044632086851
VAL_F1 : 0.8208616775110166

EPOCH : 39 / 50
VAL_LOSS : 0.29186381600224054 
VAL_ACCURACY : 0.9107358262967431
VAL_F1 : 0.8302752288628378

EPOCH : 40 / 50
VAL_LOSS : 0.284224866101375 
VAL_ACCURACY : 0.9059107358262968
VAL_F1 : 0.8243243238316594

EPOCH : 41 / 50
VAL_LOSS : 0.2803026996552944 
VAL_ACCURACY : 0.9047044632086851
VAL_F1 : 0.823266218747604

EPOCH : 42 / 50
VAL_LOSS : 0.28033020376012874 
VAL_ACCURACY : 0.9047044632086851
VAL_F1 : 0.823266218747604

EPOCH : 43 / 50
VAL_LOSS : 0.2768842022006328 
VAL_ACCURACY : 0.9143546441495778
VAL_F1 : 0.8382687922165412

EPOCH : 44 / 50
VAL_LOSS : 0.26879167871979565 
VAL_ACCURACY : 0.9119420989143546
VAL_F1 : 0.8344671196878256

EPOCH : 45 / 50
VAL_LOSS : 0.263258043390054 
VAL_ACCURACY : 0.9191797346200241
VAL_F1 : 0.8452655884187126

EPOCH : 46 / 50
VAL_LOSS : 0.260849450643246 
VAL_ACCURACY : 0.916767189384801
VAL_F1 : 0.8421052626631549

EPOCH : 47 / 50
VAL_LOSS : 0.2569467078607816 
VAL_ACCURACY : 0.916767189384801
VAL_F1 : 0.8421052626631549

EPOCH : 48 / 50
VAL_LOSS : 0.24814975175719994 
VAL_ACCURACY : 0.9191797346200241
VAL_F1 : 0.8466819217020355

EPOCH : 49 / 50
VAL_LOSS : 0.2476817094362699 
VAL_ACCURACY : 0.9143546441495778
VAL_F1 : 0.8397291191457995

EPOCH : 50 / 50
VAL_LOSS : 0.2415191764728381 
VAL_ACCURACY : 0.9203860072376358
VAL_F1 : 0.8493150679986552

# 5話-0
# 文: 夏目漱石だよ〜 Aくんはどんな本を読むの?
正解 : 0 , 予測 : 1 / 元クラス : 喜楽
tensor([[-1.2066,  0.0863]], device='cuda:0')
# 5話-0
# 文: 今ハマってる本はこの二冊…
正解 : 1 , 予測 : 0 / 元クラス : ニュートラル
tensor([[ 0.7494, -0.0361]], device='cuda:0')
# 5話-0
# 文: 少々マニアックだろうか…
正解 : 1 , 予測 : 0 / 元クラス : ニュートラル
tensor([[0.9579, 0.0161]], device='cuda:0')
# 5話-0
# 文: Bさん、苦手そうだよな…
正解 : 1 , 予測 : 0 / 元クラス : 悲哀
tensor([[ 1.1752, -0.0735]], device='cuda:0')
# 5話-0
# 文: そうなんだ〜
正解 : 1 , 予測 : 0 / 元クラス : 憤怒
tensor([[ 0.0145, -0.2790]], device='cuda:0')
# 5話-1
# 文: これ読んだよ
正解 : 1 , 予測 : 0 / 元クラス : ニュートラル
tensor([[ 1.2961, -0.6858]], device='cuda:0')
# 5話-1
# 文: ありがとうございます… それ、何読んでるんですか?
正解 : 1 , 予測 : 0 / 元クラス : 恐怖
tensor([[-0.3318, -0.5634]], device='cuda:0')
# 5話-1
# 文: 夏目漱石だよ〜 Aくんはどんな本を読むの?
正解 : 0 , 予測 : 1 / 元クラス : 喜楽
tensor([[-0.8496, -0.0081]], device='cuda:0')
# 5話-1
# 文: そうなんだ〜
正解 : 1 , 予測 : 0 / 元クラス : 驚愕
tensor([[ 0.4989, -0.2883]], device='cuda:0')
# 5話-1
# 文: 本当に今ハマってる本はこの二冊…
正解 : 1 , 予測 : 0 / 元クラス : ニュートラル
tensor([[ 1.3455, -0.6741]], device='cuda:0')
# 6話-0
# 文: ほとんど冷凍食品詰めただけだよ
正解 : 1 , 予測 : 0 / 元クラス : ニュートラル
tensor([[-0.2154, -0.4223]], device='cuda:0')
# 6話-0
# 文: キャー!
正解 : 1 , 予測 : 0 / 元クラス : 驚愕
tensor([[ 1.8927, -0.5219]], device='cuda:0')
# 6話-0
# 文: ベタだなあ
正解 : 1 , 予測 : 0 / 元クラス : 悲哀
tensor([[ 1.6996, -0.4067]], device='cuda:0')
# 6話-1
# 文: キャー!
正解 : 1 , 予測 : 0 / 元クラス : 驚愕
tensor([[ 1.8089, -0.4356]], device='cuda:0')
# 6話-1
# 文: ベタだなあ
正解 : 1 , 予測 : 0 / 元クラス : 悲哀
tensor([[ 1.5933, -0.3171]], device='cuda:0')
# 6話-1
# 文: 弁当なの珍しいですね
正解 : 1 , 予測 : 0 / 元クラス : 悲哀
tensor([[ 0.4259, -0.1253]], device='cuda:0')
# 6話-1
# 文: ええ…父の分と2つ作ったんだけど
正解 : 1 , 予測 : 0 / 元クラス : 悲哀
tensor([[ 0.1135, -0.1418]], device='cuda:0')
# 6話-1
# 文: 俺も今日弁当なんでおかず分けますよ
正解 : 1 , 予測 : 0 / 元クラス : 悲哀
tensor([[ 0.3698, -0.0559]], device='cuda:0')
# 6話-1
# 文: ありがとう…
正解 : 1 , 予測 : 0 / 元クラス : 悲哀
tensor([[ 0.7135, -0.6704]], device='cuda:0')
# 7話-0
# 文: どれがいいですか?
正解 : 0 , 予測 : 1 / 元クラス : 喜楽
tensor([[-0.1655,  0.0464]], device='cuda:0')
# 7話-0
# 文: チョコは一つしかないし、悪いよ〜
正解 : 1 , 予測 : 0 / 元クラス : 悲哀
tensor([[ 0.0488, -0.0164]], device='cuda:0')
# 7話-0
# 文: ジャーン!パフェを作ってみました〜!
正解 : 0 , 予測 : 1 / 元クラス : 喜楽
tensor([[-0.5662,  0.0284]], device='cuda:0')
# 7話-1
# 文: ジャーン!パフェを作ってみました〜!
正解 : 0 , 予測 : 1 / 元クラス : 喜楽
tensor([[-0.3286,  0.1113]], device='cuda:0')
# 7話-1
# 文: チョコ一つしかないねはんぶんこしよっか
正解 : 1 , 予測 : 0 / 元クラス : ニュートラル
tensor([[0.2250, 0.0127]], device='cuda:0')
# 8話-0
# 文: 今日は良い実験結果が出たな
正解 : 0 , 予測 : 1 / 元クラス : 喜楽
tensor([[-0.3812,  0.1624]], device='cuda:0')
# 8話-0
# 文: 国際会議でスペインにいるBさんからだ
正解 : 1 , 予測 : 0 / 元クラス : 驚愕
tensor([[ 0.1026, -0.1553]], device='cuda:0')
# 8話-0
# 文: ここは日本…何も見ていない
正解 : 1 , 予測 : 0 / 元クラス : ニュートラル
tensor([[ 0.1859, -0.0708]], device='cuda:0')
# 8話-1
# 文: ここは日本…何も見ていない
正解 : 1 , 予測 : 0 / 元クラス : ニュートラル
tensor([[ 0.2437, -0.1235]], device='cuda:0')
# 9話-0
# 文: Bさん… あれ?
正解 : 1 , 予測 : 0 / 元クラス : 驚愕
tensor([[ 0.1393, -0.2443]], device='cuda:0')
# 9話-0
# 文: Bさ…ちょっと!
正解 : 1 , 予測 : 0 / 元クラス : 悲哀
tensor([[ 0.0939, -0.1564]], device='cuda:0')
# 9話-0
# 文: ?
正解 : 1 , 予測 : 0 / 元クラス : 驚愕
tensor([[-0.0926, -0.2239]], device='cuda:0')
# 9話-0
# 文: 人違い!?
正解 : 1 , 予測 : 0 / 元クラス : 驚愕
tensor([[ 0.0585, -0.3061]], device='cuda:0')
# 9話-1
# 文: ?
正解 : 1 , 予測 : 0 / 元クラス : 驚愕
tensor([[0.5226, 0.0560]], device='cuda:0')
# 9話-1
# 文: 人違い!?
正解 : 1 , 予測 : 0 / 元クラス : 驚愕
tensor([[ 0.6541, -0.0606]], device='cuda:0')
# 9話-1
# 文: ここは よその研究室…!?
正解 : 1 , 予測 : 0 / 元クラス : 驚愕
tensor([[ 0.8628, -0.1922]], device='cuda:0')
# 9話-1
# 文: 恥ずかしい…
正解 : 1 , 予測 : 0 / 元クラス : 悲哀
tensor([[-0.0478, -0.2587]], device='cuda:0')
# 9話-1
# 文: ?
正解 : 1 , 予測 : 0 / 元クラス : 驚愕
tensor([[ 0.1228, -0.2582]], device='cuda:0')
------------------------test acc------------------------
Test Acc : 0.4308
correct: 28, total: 65
------------------------------------------------
                  喜楽        その他  accuracy  macro avg  weighted avg
precision   0.205128   0.769231  0.430769   0.487179      0.647732
recall      0.571429   0.392157  0.430769   0.481793      0.430769
f1-score    0.301887   0.519481  0.430769   0.410684      0.472614
support    14.000000  51.000000  0.430769  65.000000     65.000000
正例のF1値 : 0.30188679205268776
class weight : tensor([0.7557, 0.1616])
best:lr 1.4040205672300876e-06
EPOCH : 1 / 50
VAL_LOSS : 0.5669121530193549 
VAL_ACCURACY : 0.6996381182147166
VAL_F1 : 0.5743589739196143

EPOCH : 2 / 50
VAL_LOSS : 0.49922594886559707 
VAL_ACCURACY : 0.7201447527141134
VAL_F1 : 0.6027397255875457

EPOCH : 3 / 50
VAL_LOSS : 0.44964374086031544 
VAL_ACCURACY : 0.7080820265379976
VAL_F1 : 0.5939597311085255

EPOCH : 4 / 50
VAL_LOSS : 0.4071204031889255 
VAL_ACCURACY : 0.8069963811821471
VAL_F1 : 0.6862745093347251

EPOCH : 5 / 50
VAL_LOSS : 0.3804628232923838 
VAL_ACCURACY : 0.8335343787696019
VAL_F1 : 0.7160493822377941

EPOCH : 6 / 50
VAL_LOSS : 0.34632326662540436 
VAL_ACCURACY : 0.8552472858866104
VAL_F1 : 0.7457627113810776

EPOCH : 7 / 50
VAL_LOSS : 0.3196961828149282 
VAL_ACCURACY : 0.8673100120627262
VAL_F1 : 0.7689075625431732

EPOCH : 8 / 50
VAL_LOSS : 0.29380357924562234 
VAL_ACCURACY : 0.8914354644149578
VAL_F1 : 0.8026315784584777

EPOCH : 9 / 50
VAL_LOSS : 0.2699160914008434 
VAL_ACCURACY : 0.9324487334137516
VAL_F1 : 0.8647342990165116

EPOCH : 10 / 50
VAL_LOSS : 0.25095643676244295 
VAL_ACCURACY : 0.9252110977080821
VAL_F1 : 0.8551401864187156

EPOCH : 11 / 50
VAL_LOSS : 0.23498069967788 
VAL_ACCURACY : 0.9372738238841978
VAL_F1 : 0.8743961347652571

EPOCH : 12 / 50
VAL_LOSS : 0.22179231190910706 
VAL_ACCURACY : 0.9372738238841978
VAL_F1 : 0.8755980856247911

EPOCH : 13 / 50
VAL_LOSS : 0.20838814042508602 
VAL_ACCURACY : 0.9336550060313631
VAL_F1 : 0.8699763588396068

EPOCH : 14 / 50
VAL_LOSS : 0.19213941779274207 
VAL_ACCURACY : 0.9396863691194209
VAL_F1 : 0.87922705263963

EPOCH : 15 / 50
VAL_LOSS : 0.18359185483020085 
VAL_ACCURACY : 0.9445114595898673
VAL_F1 : 0.8878048775474955

EPOCH : 16 / 50
VAL_LOSS : 0.17397473795482746 
VAL_ACCURACY : 0.9469240048250904
VAL_F1 : 0.8932038829942148

EPOCH : 17 / 50
VAL_LOSS : 0.16659184373342073 
VAL_ACCURACY : 0.9493365500603136
VAL_F1 : 0.8980582519262301

EPOCH : 18 / 50
VAL_LOSS : 0.15948070571399653 
VAL_ACCURACY : 0.9529553679131484
VAL_F1 : 0.9037037032014632

EPOCH : 19 / 50
VAL_LOSS : 0.15021522830312067 
VAL_ACCURACY : 0.9553679131483716
VAL_F1 : 0.9086419748063771

EPOCH : 20 / 50
VAL_LOSS : 0.13919413605561623 
VAL_ACCURACY : 0.9517490952955368
VAL_F1 : 0.9024390238888876

EPOCH : 21 / 50
VAL_LOSS : 0.1338852676921166 
VAL_ACCURACY : 0.9565741857659831
VAL_F1 : 0.9113300487589727

EPOCH : 22 / 50
VAL_LOSS : 0.12678677380944675 
VAL_ACCURACY : 0.962605548854041
VAL_F1 : 0.9226932663299607

EPOCH : 23 / 50
VAL_LOSS : 0.12340895380251683 
VAL_ACCURACY : 0.9734620024125452
VAL_F1 : 0.9441624360442553

EPOCH : 24 / 50
VAL_LOSS : 0.1214228099785172 
VAL_ACCURACY : 0.9746682750301568
VAL_F1 : 0.9462915595980143

EPOCH : 25 / 50
VAL_LOSS : 0.11215695122686717 
VAL_ACCURACY : 0.9782870928829915
VAL_F1 : 0.9538461533417357

EPOCH : 26 / 50
VAL_LOSS : 0.10558877032823287 
VAL_ACCURACY : 0.9794933655006032
VAL_F1 : 0.956298200009622

EPOCH : 27 / 50
VAL_LOSS : 0.1003597487982076 
VAL_ACCURACY : 0.9782870928829915
VAL_F1 : 0.9540816321488312

EPOCH : 28 / 50
VAL_LOSS : 0.09733733312728313 
VAL_ACCURACY : 0.9831121833534379
VAL_F1 : 0.9639175252685594

EPOCH : 29 / 50
VAL_LOSS : 0.09145864525523323 
VAL_ACCURACY : 0.985524728588661
VAL_F1 : 0.9687499994950766

EPOCH : 30 / 50
VAL_LOSS : 0.09190752228292134 
VAL_ACCURACY : 0.9819059107358263
VAL_F1 : 0.9614395881844027

EPOCH : 31 / 50
VAL_LOSS : 0.08595870936719271 
VAL_ACCURACY : 0.9843184559710495
VAL_F1 : 0.9664082682291262

EPOCH : 32 / 50
VAL_LOSS : 0.08233399757255729 
VAL_ACCURACY : 0.9819059107358263
VAL_F1 : 0.9614395881844027

EPOCH : 33 / 50
VAL_LOSS : 0.07914604980928394 
VAL_ACCURACY : 0.9867310012062727
VAL_F1 : 0.9712793728631596

EPOCH : 34 / 50
VAL_LOSS : 0.07549319204945977 
VAL_ACCURACY : 0.9879372738238842
VAL_F1 : 0.9739583328283827

EPOCH : 35 / 50
VAL_LOSS : 0.07308880862994836 
VAL_ACCURACY : 0.9879372738238842
VAL_F1 : 0.9739583328283827

EPOCH : 36 / 50
VAL_LOSS : 0.07468371465802193 
VAL_ACCURACY : 0.9879372738238842
VAL_F1 : 0.9739583328283827

EPOCH : 37 / 50
VAL_LOSS : 0.07008898745362575 
VAL_ACCURACY : 0.9879372738238842
VAL_F1 : 0.9739583328283827

EPOCH : 38 / 50
VAL_LOSS : 0.06351175710845453 
VAL_ACCURACY : 0.9891435464414958
VAL_F1 : 0.9765013049780148

EPOCH : 39 / 50
VAL_LOSS : 0.06778360111638904 
VAL_ACCURACY : 0.9891435464414958
VAL_F1 : 0.9765013049780148

EPOCH : 40 / 50
VAL_LOSS : 0.06300922979314166 
VAL_ACCURACY : 0.9903498190591074
VAL_F1 : 0.979166666161689

EPOCH : 41 / 50
VAL_LOSS : 0.06248823800482429 
VAL_ACCURACY : 0.9903498190591074
VAL_F1 : 0.979166666161689

EPOCH : 42 / 50
VAL_LOSS : 0.057665109885140106 
VAL_ACCURACY : 0.9879372738238842
VAL_F1 : 0.9740932637438724

EPOCH : 43 / 50
VAL_LOSS : 0.058331428812100336 
VAL_ACCURACY : 0.9903498190591074
VAL_F1 : 0.979166666161689

EPOCH : 44 / 50
VAL_LOSS : 0.06026715095728063 
VAL_ACCURACY : 0.9891435464414958
VAL_F1 : 0.9765013049780148

EPOCH : 45 / 50
VAL_LOSS : 0.05715653588637137 
VAL_ACCURACY : 0.9879372738238842
VAL_F1 : 0.9740932637438724

EPOCH : 46 / 50
VAL_LOSS : 0.060187367238820746 
VAL_ACCURACY : 0.9879372738238842
VAL_F1 : 0.9740932637438724

EPOCH : 47 / 50
VAL_LOSS : 0.05188942215262124 
VAL_ACCURACY : 0.9891435464414958
VAL_F1 : 0.9766233761184686

EPOCH : 48 / 50
VAL_LOSS : 0.0540419548140982 
VAL_ACCURACY : 0.9903498190591074
VAL_F1 : 0.979166666161689

EPOCH : 49 / 50
VAL_LOSS : 0.049681072463639654 
VAL_ACCURACY : 0.9903498190591074
VAL_F1 : 0.979166666161689

EPOCH : 50 / 50
VAL_LOSS : 0.04777304754735759 
VAL_ACCURACY : 0.9891435464414958
VAL_F1 : 0.9766233761184686

# 5話-0
# 文: 夏目漱石だよ〜 Aくんはどんな本を読むの?
正解 : 0 , 予測 : 1 / 元クラス : 喜楽
tensor([[-1.6949,  1.8174]], device='cuda:0')
# 5話-0
# 文: Bさん、苦手そうだよな…
正解 : 1 , 予測 : 0 / 元クラス : 悲哀
tensor([[ 0.0866, -0.1136]], device='cuda:0')
# 5話-0
# 文: そうなんだ〜
正解 : 1 , 予測 : 0 / 元クラス : 憤怒
tensor([[ 0.3727, -0.1546]], device='cuda:0')
# 5話-1
# 文: これ読んだよ
正解 : 1 , 予測 : 0 / 元クラス : ニュートラル
tensor([[ 1.8205, -2.1805]], device='cuda:0')
# 5話-1
# 文: ありがとうございます… それ、何読んでるんですか?
正解 : 1 , 予測 : 0 / 元クラス : 恐怖
tensor([[ 1.0360, -1.2919]], device='cuda:0')
# 5話-1
# 文: 夏目漱石だよ〜 Aくんはどんな本を読むの?
正解 : 0 , 予測 : 1 / 元クラス : 喜楽
tensor([[-1.4280,  1.5428]], device='cuda:0')
# 5話-1
# 文: そうなんだ〜
正解 : 1 , 予測 : 0 / 元クラス : 驚愕
tensor([[ 0.7641, -0.5198]], device='cuda:0')
# 5話-1
# 文: 本当に今ハマってる本はこの二冊…
正解 : 1 , 予測 : 0 / 元クラス : ニュートラル
tensor([[ 0.7770, -1.1374]], device='cuda:0')
# 6話-0
# 文: ほとんど冷凍食品詰めただけだよ
正解 : 1 , 予測 : 0 / 元クラス : ニュートラル
tensor([[ 0.2903, -0.5675]], device='cuda:0')
# 6話-0
# 文: 見てもいいですか?
正解 : 0 , 予測 : 1 / 元クラス : 喜楽
tensor([[-0.3236,  0.2055]], device='cuda:0')
# 6話-0
# 文: キャー!
正解 : 1 , 予測 : 0 / 元クラス : 驚愕
tensor([[ 1.5225, -2.0526]], device='cuda:0')
# 6話-0
# 文: ベタだなあ
正解 : 1 , 予測 : 0 / 元クラス : 悲哀
tensor([[ 0.9100, -1.4360]], device='cuda:0')
# 6話-1
# 文: キャー!
正解 : 1 , 予測 : 0 / 元クラス : 驚愕
tensor([[ 1.4355, -1.8679]], device='cuda:0')
# 6話-1
# 文: ベタだなあ
正解 : 1 , 予測 : 0 / 元クラス : 悲哀
tensor([[ 0.7846, -1.1636]], device='cuda:0')
# 6話-1
# 文: ありがとう…
正解 : 1 , 予測 : 0 / 元クラス : 悲哀
tensor([[ 1.5265, -1.5101]], device='cuda:0')
# 7話-0
# 文: どれがいいですか?
正解 : 0 , 予測 : 1 / 元クラス : 喜楽
tensor([[-1.0433,  0.5384]], device='cuda:0')
# 7話-0
# 文: 別に気にしませんが…
正解 : 1 , 予測 : 0 / 元クラス : ニュートラル
tensor([[ 0.5301, -0.5996]], device='cuda:0')
# 7話-0
# 文: ジャーン!パフェを作ってみました〜!
正解 : 0 , 予測 : 1 / 元クラス : 喜楽
tensor([[-0.0293,  0.4932]], device='cuda:0')
# 7話-1
# 文: ジャーン!パフェを作ってみました〜!
正解 : 0 , 予測 : 1 / 元クラス : 喜楽
tensor([[0.0781, 0.2498]], device='cuda:0')
# 7話-1
# 文: チョコ一つしかないねはんぶんこしよっか
正解 : 1 , 予測 : 0 / 元クラス : ニュートラル
tensor([[ 0.0964, -0.5642]], device='cuda:0')
# 8話-0
# 文: 今日は良い実験結果が出たな
正解 : 0 , 予測 : 1 / 元クラス : 喜楽
tensor([[-1.5677,  1.7871]], device='cuda:0')
# 8話-0
# 文: 国際会議でスペインにいるBさんからだ
正解 : 1 , 予測 : 0 / 元クラス : 驚愕
tensor([[ 0.9858, -1.1214]], device='cuda:0')
# 9話-1
# 文: ここは よその研究室…!?
正解 : 1 , 予測 : 0 / 元クラス : 驚愕
tensor([[ 1.1532, -1.2989]], device='cuda:0')
# 9話-1
# 文: おかえりなさい!
正解 : 0 , 予測 : 1 / 元クラス : 喜楽
tensor([[-0.1829,  0.1937]], device='cuda:0')
------------------------test acc------------------------
Test Acc : 0.6308
correct: 41, total: 65
------------------------------------------------
                  喜楽        その他  accuracy  macro avg  weighted avg
precision   0.272727   0.813953  0.630769   0.543340      0.697382
recall      0.428571   0.686275  0.630769   0.557423      0.630769
f1-score    0.333333   0.744681  0.630769   0.539007      0.656083
support    14.000000  51.000000  0.630769  65.000000     65.000000
正例のF1値 : 0.33333333283950617
class weight : tensor([0.7557, 0.1616])
best:lr 3.4829948893791797e-06
EPOCH : 1 / 50
VAL_LOSS : 0.496043127316695 
VAL_ACCURACY : 0.5995174909529554
VAL_F1 : 0.5174418600651239

EPOCH : 2 / 50
VAL_LOSS : 0.39912171776478106 
VAL_ACCURACY : 0.8805790108564535
VAL_F1 : 0.7804878043876875

EPOCH : 3 / 50
VAL_LOSS : 0.34229305071326405 
VAL_ACCURACY : 0.8805790108564535
VAL_F1 : 0.7870967737076656

EPOCH : 4 / 50
VAL_LOSS : 0.2902480693390736 
VAL_ACCURACY : 0.8998793727382388
VAL_F1 : 0.8167770414527045

EPOCH : 5 / 50
VAL_LOSS : 0.24482584271866542 
VAL_ACCURACY : 0.9288299155609168
VAL_F1 : 0.8598574816864045

EPOCH : 6 / 50
VAL_LOSS : 0.2097834054953777 
VAL_ACCURACY : 0.9276236429433052
VAL_F1 : 0.8598130836149555

EPOCH : 7 / 50
VAL_LOSS : 0.18010547900429139 
VAL_ACCURACY : 0.9372738238841978
VAL_F1 : 0.8779342718026957

EPOCH : 8 / 50
VAL_LOSS : 0.1595870588834469 
VAL_ACCURACY : 0.9529553679131484
VAL_F1 : 0.9051094885499139

EPOCH : 9 / 50
VAL_LOSS : 0.14169151070885933 
VAL_ACCURACY : 0.9722557297949337
VAL_F1 : 0.9411764700839739

EPOCH : 10 / 50
VAL_LOSS : 0.12476388794871476 
VAL_ACCURACY : 0.9758745476477684
VAL_F1 : 0.9487179482135569

EPOCH : 11 / 50
VAL_LOSS : 0.11531910149810407 
VAL_ACCURACY : 0.9782870928829915
VAL_F1 : 0.9543147203081631

EPOCH : 12 / 50
VAL_LOSS : 0.10551128156769735 
VAL_ACCURACY : 0.9794933655006032
VAL_F1 : 0.9565217386260949

EPOCH : 13 / 50
VAL_LOSS : 0.09515008731530263 
VAL_ACCURACY : 0.9819059107358263
VAL_F1 : 0.9616368281401352

EPOCH : 14 / 50
VAL_LOSS : 0.08647385253929175 
VAL_ACCURACY : 0.985524728588661
VAL_F1 : 0.9690721644437906

EPOCH : 15 / 50
VAL_LOSS : 0.08251867790778096 
VAL_ACCURACY : 0.9806996381182147
VAL_F1 : 0.9591836729651319

EPOCH : 16 / 50
VAL_LOSS : 0.07376982751660623 
VAL_ACCURACY : 0.9879372738238842
VAL_F1 : 0.9740932637438724

EPOCH : 17 / 50
VAL_LOSS : 0.07068979779544932 
VAL_ACCURACY : 0.9867310012062727
VAL_F1 : 0.9715762268854303

EPOCH : 18 / 50
VAL_LOSS : 0.06507834304983799 
VAL_ACCURACY : 0.9891435464414958
VAL_F1 : 0.9766233761184686

EPOCH : 19 / 50
VAL_LOSS : 0.05979666382504197 
VAL_ACCURACY : 0.9903498190591074
VAL_F1 : 0.979166666161689

EPOCH : 20 / 50
VAL_LOSS : 0.05840978925474561 
VAL_ACCURACY : 0.9903498190591074
VAL_F1 : 0.979166666161689

EPOCH : 21 / 50
VAL_LOSS : 0.05883560404897882 
VAL_ACCURACY : 0.9903498190591074
VAL_F1 : 0.979166666161689

EPOCH : 22 / 50
VAL_LOSS : 0.05232606276583213 
VAL_ACCURACY : 0.9891435464414958
VAL_F1 : 0.9766233761184686

EPOCH : 23 / 50
VAL_LOSS : 0.04787439924593155 
VAL_ACCURACY : 0.991556091676719
VAL_F1 : 0.98172323709287

EPOCH : 24 / 50
VAL_LOSS : 0.050131897119661935 
VAL_ACCURACY : 0.9891435464414958
VAL_F1 : 0.9766233761184686

EPOCH : 25 / 50
VAL_LOSS : 0.04854992100109275 
VAL_ACCURACY : 0.9903498190591074
VAL_F1 : 0.979166666161689

EPOCH : 26 / 50
VAL_LOSS : 0.045036651068725266 
VAL_ACCURACY : 0.991556091676719
VAL_F1 : 0.98172323709287

EPOCH : 27 / 50
VAL_LOSS : 0.04739989345678343 
VAL_ACCURACY : 0.9903498190591074
VAL_F1 : 0.979166666161689

EPOCH : 28 / 50
VAL_LOSS : 0.04472099156835331 
VAL_ACCURACY : 0.991556091676719
VAL_F1 : 0.98172323709287

EPOCH : 29 / 50
VAL_LOSS : 0.039364299145885386 
VAL_ACCURACY : 0.991556091676719
VAL_F1 : 0.98172323709287

EPOCH : 30 / 50
VAL_LOSS : 0.045683806511358574 
VAL_ACCURACY : 0.991556091676719
VAL_F1 : 0.98172323709287

EPOCH : 31 / 50
VAL_LOSS : 0.03996697635962986 
VAL_ACCURACY : 0.991556091676719
VAL_F1 : 0.98172323709287

EPOCH : 32 / 50
VAL_LOSS : 0.04014672587912243 
VAL_ACCURACY : 0.9891435464414958
VAL_F1 : 0.9766233761184686

EPOCH : 33 / 50
VAL_LOSS : 0.03799462553042059 
VAL_ACCURACY : 0.991556091676719
VAL_F1 : 0.98172323709287

EPOCH : 34 / 50
VAL_LOSS : 0.03677135501773311 
VAL_ACCURACY : 0.991556091676719
VAL_F1 : 0.98172323709287

EPOCH : 35 / 50
VAL_LOSS : 0.03439763514325023 
VAL_ACCURACY : 0.9903498190591074
VAL_F1 : 0.979166666161689

EPOCH : 36 / 50
VAL_LOSS : 0.03595940233208239 
VAL_ACCURACY : 0.9891435464414958
VAL_F1 : 0.9766233761184686

EPOCH : 37 / 50
VAL_LOSS : 0.03908415699306016 
VAL_ACCURACY : 0.991556091676719
VAL_F1 : 0.98172323709287

EPOCH : 38 / 50
VAL_LOSS : 0.03164177366460745 
VAL_ACCURACY : 0.9903498190591074
VAL_F1 : 0.979166666161689

EPOCH : 39 / 50
VAL_LOSS : 0.03289010271859857 
VAL_ACCURACY : 0.9903498190591074
VAL_F1 : 0.979166666161689

EPOCH : 40 / 50
VAL_LOSS : 0.029744352840890106 
VAL_ACCURACY : 0.991556091676719
VAL_F1 : 0.98172323709287

EPOCH : 41 / 50
VAL_LOSS : 0.02937190872258865 
VAL_ACCURACY : 0.991556091676719
VAL_F1 : 0.98172323709287

EPOCH : 42 / 50
VAL_LOSS : 0.028511928251156442 
VAL_ACCURACY : 0.9891435464414958
VAL_F1 : 0.9766233761184686

EPOCH : 43 / 50
VAL_LOSS : 0.027848466424844585 
VAL_ACCURACY : 0.991556091676719
VAL_F1 : 0.98172323709287

EPOCH : 44 / 50
VAL_LOSS : 0.0275869339865704 
VAL_ACCURACY : 0.9891435464414958
VAL_F1 : 0.9766233761184686

EPOCH : 45 / 50
VAL_LOSS : 0.028384881031412918 
VAL_ACCURACY : 0.9891435464414958
VAL_F1 : 0.9766233761184686

EPOCH : 46 / 50
VAL_LOSS : 0.024882816405107196 
VAL_ACCURACY : 0.9903498190591074
VAL_F1 : 0.979166666161689

EPOCH : 47 / 50
VAL_LOSS : 0.02447556223397931 
VAL_ACCURACY : 0.9891435464414958
VAL_F1 : 0.9766233761184686

EPOCH : 48 / 50
VAL_LOSS : 0.02597025828436017 
VAL_ACCURACY : 0.9891435464414958
VAL_F1 : 0.9766233761184686

EPOCH : 49 / 50
VAL_LOSS : 0.023679976297829013 
VAL_ACCURACY : 0.9891435464414958
VAL_F1 : 0.9766233761184686

EPOCH : 50 / 50
VAL_LOSS : 0.025911014589767616 
VAL_ACCURACY : 0.9891435464414958
VAL_F1 : 0.9766233761184686

# 5話-0
# 文: 夏目漱石だよ〜 Aくんはどんな本を読むの?
正解 : 0 , 予測 : 1 / 元クラス : 喜楽
tensor([[-2.4126,  1.8183]], device='cuda:0')
# 5話-1
# 文: これ読んだよ
正解 : 1 , 予測 : 0 / 元クラス : ニュートラル
tensor([[ 2.2155, -1.7655]], device='cuda:0')
# 5話-1
# 文: ありがとうございます… それ、何読んでるんですか?
正解 : 1 , 予測 : 0 / 元クラス : 恐怖
tensor([[ 1.0536, -1.1077]], device='cuda:0')
# 5話-1
# 文: 夏目漱石だよ〜 Aくんはどんな本を読むの?
正解 : 0 , 予測 : 1 / 元クラス : 喜楽
tensor([[-2.3025,  1.6579]], device='cuda:0')
# 5話-1
# 文: そうなんだ〜
正解 : 1 , 予測 : 0 / 元クラス : 驚愕
tensor([[ 0.1690, -0.3023]], device='cuda:0')
# 5話-1
# 文: 本当に今ハマってる本はこの二冊…
正解 : 1 , 予測 : 0 / 元クラス : ニュートラル
tensor([[ 0.4465, -0.3759]], device='cuda:0')
# 6話-0
# 文: キャー!
正解 : 1 , 予測 : 0 / 元クラス : 驚愕
tensor([[ 1.8429, -1.4496]], device='cuda:0')
# 6話-0
# 文: ベタだなあ
正解 : 1 , 予測 : 0 / 元クラス : 悲哀
tensor([[ 0.9176, -0.6750]], device='cuda:0')
# 6話-1
# 文: キャー!
正解 : 1 , 予測 : 0 / 元クラス : 驚愕
tensor([[ 1.5083, -1.2513]], device='cuda:0')
# 6話-1
# 文: ベタだなあ
正解 : 1 , 予測 : 0 / 元クラス : 悲哀
tensor([[ 0.4414, -0.3801]], device='cuda:0')
# 6話-1
# 文: 俺も今日弁当なんでおかず分けますよ
正解 : 1 , 予測 : 0 / 元クラス : 悲哀
tensor([[ 0.2663, -0.0687]], device='cuda:0')
# 6話-1
# 文: ありがとう…
正解 : 1 , 予測 : 0 / 元クラス : 悲哀
tensor([[ 1.7823, -1.3820]], device='cuda:0')
# 7話-0
# 文: どれがいいですか?
正解 : 0 , 予測 : 1 / 元クラス : 喜楽
tensor([[-1.5588,  1.1844]], device='cuda:0')
# 7話-0
# 文: 別に気にしませんが…
正解 : 1 , 予測 : 0 / 元クラス : ニュートラル
tensor([[ 0.1366, -0.6138]], device='cuda:0')
# 7話-0
# 文: ジャーン!パフェを作ってみました〜!
正解 : 0 , 予測 : 1 / 元クラス : 喜楽
tensor([[-0.7162,  0.2167]], device='cuda:0')
# 7話-1
# 文: ジャーン!パフェを作ってみました〜!
正解 : 0 , 予測 : 1 / 元クラス : 喜楽
tensor([[-0.4061,  0.0989]], device='cuda:0')
# 7話-1
# 文: チョコ一つしかないねはんぶんこしよっか
正解 : 1 , 予測 : 0 / 元クラス : ニュートラル
tensor([[ 0.0679, -0.1121]], device='cuda:0')
# 8話-0
# 文: 今日は良い実験結果が出たな
正解 : 0 , 予測 : 1 / 元クラス : 喜楽
tensor([[-2.1439,  1.6279]], device='cuda:0')
# 8話-0
# 文: 国際会議でスペインにいるBさんからだ
正解 : 1 , 予測 : 0 / 元クラス : 驚愕
tensor([[ 1.2173, -0.8846]], device='cuda:0')
# 9話-1
# 文: ここは よその研究室…!?
正解 : 1 , 予測 : 0 / 元クラス : 驚愕
tensor([[ 1.4482, -1.2913]], device='cuda:0')
# 9話-1
# 文: おかえりなさい!
正解 : 0 , 予測 : 1 / 元クラス : 喜楽
tensor([[-0.4804,  0.2603]], device='cuda:0')
------------------------test acc------------------------
Test Acc : 0.6769
correct: 44, total: 65
------------------------------------------------
                  喜楽        その他  accuracy  macro avg  weighted avg
precision   0.333333   0.840909  0.676923   0.587121      0.731585
recall      0.500000   0.725490  0.676923   0.612745      0.676923
f1-score    0.400000   0.778947  0.676923   0.589474      0.697328
support    14.000000  51.000000  0.676923  65.000000     65.000000
正例のF1値 : 0.3999999994971428
class weight : tensor([0.7557, 0.1616])
best:lr 5.766896107621153e-06
EPOCH : 1 / 50
VAL_LOSS : 0.4567974823025557 
VAL_ACCURACY : 0.850422195416164
VAL_F1 : 0.7268722462068448

EPOCH : 2 / 50
VAL_LOSS : 0.3581854274066595 
VAL_ACCURACY : 0.8781664656212304
VAL_F1 : 0.7770419421151316

EPOCH : 3 / 50
VAL_LOSS : 0.26980051369621205 
VAL_ACCURACY : 0.9203860072376358
VAL_F1 : 0.8472222217261125

EPOCH : 4 / 50
VAL_LOSS : 0.21794802881777287 
VAL_ACCURACY : 0.9191797346200241
VAL_F1 : 0.8480725618646348

EPOCH : 5 / 50
VAL_LOSS : 0.17334826820744917 
VAL_ACCURACY : 0.9312424607961399
VAL_F1 : 0.8677494194571306

EPOCH : 6 / 50
VAL_LOSS : 0.1432828431805739 
VAL_ACCURACY : 0.97708082026538
VAL_F1 : 0.9516539435162417

EPOCH : 7 / 50
VAL_LOSS : 0.1267329162129989 
VAL_ACCURACY : 0.9794933655006032
VAL_F1 : 0.9556135765185938

EPOCH : 8 / 50
VAL_LOSS : 0.11131140811798665 
VAL_ACCURACY : 0.9831121833534379
VAL_F1 : 0.9635416661617704

EPOCH : 9 / 50
VAL_LOSS : 0.10363141389993522 
VAL_ACCURACY : 0.971049457177322
VAL_F1 : 0.9399999994968126

EPOCH : 10 / 50
VAL_LOSS : 0.08475470987076943 
VAL_ACCURACY : 0.9891435464414958
VAL_F1 : 0.9765013049780148

EPOCH : 11 / 50
VAL_LOSS : 0.0775189301572167 
VAL_ACCURACY : 0.9891435464414958
VAL_F1 : 0.9766233761184686

EPOCH : 12 / 50
VAL_LOSS : 0.07172992875656256 
VAL_ACCURACY : 0.9891435464414958
VAL_F1 : 0.9766233761184686

EPOCH : 13 / 50
VAL_LOSS : 0.06508224102883385 
VAL_ACCURACY : 0.9879372738238842
VAL_F1 : 0.9740932637438724

EPOCH : 14 / 50
VAL_LOSS : 0.05881468618575197 
VAL_ACCURACY : 0.9891435464414958
VAL_F1 : 0.9765013049780148

EPOCH : 15 / 50
VAL_LOSS : 0.057086135511501476 
VAL_ACCURACY : 0.9891435464414958
VAL_F1 : 0.9766233761184686

EPOCH : 16 / 50
VAL_LOSS : 0.0538540853259082 
VAL_ACCURACY : 0.991556091676719
VAL_F1 : 0.98172323709287

EPOCH : 17 / 50
VAL_LOSS : 0.058748813023647435 
VAL_ACCURACY : 0.991556091676719
VAL_F1 : 0.98172323709287

EPOCH : 18 / 50
VAL_LOSS : 0.05083832828901135 
VAL_ACCURACY : 0.9879372738238842
VAL_F1 : 0.9740932637438724

EPOCH : 19 / 50
VAL_LOSS : 0.05100519571883174 
VAL_ACCURACY : 0.9867310012062727
VAL_F1 : 0.9715762268854303

EPOCH : 20 / 50
VAL_LOSS : 0.04614476698379104 
VAL_ACCURACY : 0.991556091676719
VAL_F1 : 0.98172323709287

EPOCH : 21 / 50
VAL_LOSS : 0.044420103900707684 
VAL_ACCURACY : 0.9903498190591074
VAL_F1 : 0.979166666161689

EPOCH : 22 / 50
VAL_LOSS : 0.044731036939013466 
VAL_ACCURACY : 0.9891435464414958
VAL_F1 : 0.9766233761184686

EPOCH : 23 / 50
VAL_LOSS : 0.046346055487027533 
VAL_ACCURACY : 0.9891435464414958
VAL_F1 : 0.9766233761184686

EPOCH : 24 / 50
VAL_LOSS : 0.042877158956029095 
VAL_ACCURACY : 0.9891435464414958
VAL_F1 : 0.9766233761184686

EPOCH : 25 / 50
VAL_LOSS : 0.04456074876137651 
VAL_ACCURACY : 0.9891435464414958
VAL_F1 : 0.9766233761184686

EPOCH : 26 / 50
VAL_LOSS : 0.04043569345958531 
VAL_ACCURACY : 0.9903498190591074
VAL_F1 : 0.979166666161689

EPOCH : 27 / 50
VAL_LOSS : 0.04162676652105382 
VAL_ACCURACY : 0.9903498190591074
VAL_F1 : 0.979166666161689

EPOCH : 28 / 50
VAL_LOSS : 0.03910885638414094 
VAL_ACCURACY : 0.9903498190591074
VAL_F1 : 0.979166666161689

EPOCH : 29 / 50
VAL_LOSS : 0.03936762104813869 
VAL_ACCURACY : 0.9927623642943305
VAL_F1 : 0.984293193212179

EPOCH : 30 / 50
VAL_LOSS : 0.034016066875595316 
VAL_ACCURACY : 0.9903498190591074
VAL_F1 : 0.979166666161689

EPOCH : 31 / 50
VAL_LOSS : 0.03261330736299547 
VAL_ACCURACY : 0.9927623642943305
VAL_F1 : 0.984293193212179

EPOCH : 32 / 50
VAL_LOSS : 0.029804941033944488 
VAL_ACCURACY : 0.9927623642943305
VAL_F1 : 0.984293193212179

EPOCH : 33 / 50
VAL_LOSS : 0.03128511789971246 
VAL_ACCURACY : 0.9903498190591074
VAL_F1 : 0.979166666161689

EPOCH : 34 / 50
VAL_LOSS : 0.030653153743165042 
VAL_ACCURACY : 0.9927623642943305
VAL_F1 : 0.984293193212179

EPOCH : 35 / 50
VAL_LOSS : 0.030996936248042263 
VAL_ACCURACY : 0.9903498190591074
VAL_F1 : 0.979166666161689

EPOCH : 36 / 50
VAL_LOSS : 0.030759192644976653 
VAL_ACCURACY : 0.9927623642943305
VAL_F1 : 0.984293193212179

EPOCH : 37 / 50
VAL_LOSS : 0.031031836272002414 
VAL_ACCURACY : 0.9903498190591074
VAL_F1 : 0.979166666161689

EPOCH : 38 / 50
VAL_LOSS : 0.029402305783990484 
VAL_ACCURACY : 0.9903498190591074
VAL_F1 : 0.979166666161689

EPOCH : 39 / 50
VAL_LOSS : 0.025894780347768504 
VAL_ACCURACY : 0.9927623642943305
VAL_F1 : 0.984293193212179

EPOCH : 40 / 50
VAL_LOSS : 0.025344509514980018 
VAL_ACCURACY : 0.9903498190591074
VAL_F1 : 0.979166666161689

EPOCH : 41 / 50
VAL_LOSS : 0.0263026714271221 
VAL_ACCURACY : 0.9927623642943305
VAL_F1 : 0.984293193212179

EPOCH : 42 / 50
VAL_LOSS : 0.022780129990468804 
VAL_ACCURACY : 0.9927623642943305
VAL_F1 : 0.984293193212179

EPOCH : 43 / 50
VAL_LOSS : 0.021528384836318974 
VAL_ACCURACY : 0.9987937273823885
VAL_F1 : 0.9973474796008133

EPOCH : 44 / 50
VAL_LOSS : 0.02321890944865747 
VAL_ACCURACY : 0.9951749095295537
VAL_F1 : 0.9894736837053326

EPOCH : 45 / 50
VAL_LOSS : 0.019544335583654735 
VAL_ACCURACY : 0.9927623642943305
VAL_F1 : 0.984293193212179

EPOCH : 46 / 50
VAL_LOSS : 0.018324069392222624 
VAL_ACCURACY : 0.9927623642943305
VAL_F1 : 0.984293193212179

EPOCH : 47 / 50
VAL_LOSS : 0.020591698479480468 
VAL_ACCURACY : 0.9951749095295537
VAL_F1 : 0.9894736837053326

EPOCH : 48 / 50
VAL_LOSS : 0.016781473863654986 
VAL_ACCURACY : 0.9951749095295537
VAL_F1 : 0.9894736837053326

EPOCH : 49 / 50
VAL_LOSS : 0.017497605823266964 
VAL_ACCURACY : 0.9939686369119421
VAL_F1 : 0.9868766399147981

EPOCH : 50 / 50
VAL_LOSS : 0.016343779109704953 
VAL_ACCURACY : 0.9939686369119421
VAL_F1 : 0.9868766399147981

# 5話-0
# 文: 夏目漱石だよ〜 Aくんはどんな本を読むの?
正解 : 0 , 予測 : 1 / 元クラス : 喜楽
tensor([[-1.8214,  3.0193]], device='cuda:0')
# 5話-1
# 文: これ読んだよ
正解 : 1 , 予測 : 0 / 元クラス : ニュートラル
tensor([[ 1.8122, -3.1936]], device='cuda:0')
# 5話-1
# 文: ありがとうございます… それ、何読んでるんですか?
正解 : 1 , 予測 : 0 / 元クラス : 恐怖
tensor([[ 0.3424, -0.8326]], device='cuda:0')
# 5話-1
# 文: 夏目漱石だよ〜 Aくんはどんな本を読むの?
正解 : 0 , 予測 : 1 / 元クラス : 喜楽
tensor([[-1.8037,  2.9886]], device='cuda:0')
# 5話-1
# 文: 本当に今ハマってる本はこの二冊…
正解 : 1 , 予測 : 0 / 元クラス : ニュートラル
tensor([[ 0.2872, -1.1315]], device='cuda:0')
# 6話-0
# 文: 見てもいいですか?
正解 : 0 , 予測 : 1 / 元クラス : 喜楽
tensor([[-0.4419,  0.6276]], device='cuda:0')
# 6話-0
# 文: キャー!
正解 : 1 , 予測 : 0 / 元クラス : 驚愕
tensor([[ 0.4056, -1.2535]], device='cuda:0')
# 6話-0
# 文: ベタだなあ
正解 : 1 , 予測 : 0 / 元クラス : 悲哀
tensor([[ 0.5457, -1.1872]], device='cuda:0')
# 6話-1
# 文: キャー!
正解 : 1 , 予測 : 0 / 元クラス : 驚愕
tensor([[-0.0930, -0.3260]], device='cuda:0')
# 6話-1
# 文: ベタだなあ
正解 : 1 , 予測 : 0 / 元クラス : 悲哀
tensor([[ 0.0560, -0.2326]], device='cuda:0')
# 6話-1
# 文: 俺も今日弁当なんでおかず分けますよ
正解 : 1 , 予測 : 0 / 元クラス : 悲哀
tensor([[ 0.1706, -0.2276]], device='cuda:0')
# 6話-1
# 文: ありがとう…
正解 : 1 , 予測 : 0 / 元クラス : 悲哀
tensor([[ 1.7956, -3.1057]], device='cuda:0')
# 7話-0
# 文: どれがいいですか?
正解 : 0 , 予測 : 1 / 元クラス : 喜楽
tensor([[-1.7354,  2.8884]], device='cuda:0')
# 7話-0
# 文: 別に気にしませんが…
正解 : 1 , 予測 : 0 / 元クラス : ニュートラル
tensor([[ 0.1541, -0.6949]], device='cuda:0')
# 7話-0
# 文: ジャーン!パフェを作ってみました〜!
正解 : 0 , 予測 : 1 / 元クラス : 喜楽
tensor([[-0.5509,  0.7560]], device='cuda:0')
# 7話-1
# 文: ジャーン!パフェを作ってみました〜!
正解 : 0 , 予測 : 1 / 元クラス : 喜楽
tensor([[-0.3544,  0.4021]], device='cuda:0')
# 8話-0
# 文: 今日は良い実験結果が出たな
正解 : 0 , 予測 : 1 / 元クラス : 喜楽
tensor([[-1.7389,  2.8998]], device='cuda:0')
# 8話-0
# 文: 国際会議でスペインにいるBさんからだ
正解 : 1 , 予測 : 0 / 元クラス : 驚愕
tensor([[ 0.8258, -1.7041]], device='cuda:0')
# 9話-1
# 文: ここは よその研究室…!?
正解 : 1 , 予測 : 0 / 元クラス : 驚愕
tensor([[ 1.5677, -2.5437]], device='cuda:0')
# 9話-1
# 文: おかえりなさい!
正解 : 0 , 予測 : 1 / 元クラス : 喜楽
tensor([[-1.4045,  2.2301]], device='cuda:0')
------------------------test acc------------------------
Test Acc : 0.6923
correct: 45, total: 65
------------------------------------------------
                  喜楽        その他  accuracy  macro avg  weighted avg
precision   0.333333   0.829787  0.692308   0.581560      0.722859
recall      0.428571   0.764706  0.692308   0.596639      0.692308
f1-score    0.375000   0.795918  0.692308   0.585459      0.705259
support    14.000000  51.000000  0.692308  65.000000     65.000000
正例のF1値 : 0.37499999948437496
