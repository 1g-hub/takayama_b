\newpage
\changeindent{0cm}
\section{要素技術}
\changeindent{2cm}

本章では，実験に関連する要素技術について説明する．

\changeindent{0cm}
\subsection{形態素解析}
\changeindent{2cm}

形態素とは日本語などの自然言語において意味を持つ最小の単位のことであり,
文を形態素に分割し, 各形態素の品詞などを判定する技術を形態素解析という.
英語の文では, 単語と単語の区切りがほとんどの箇所で明示的に示される.
このため、形態素への分割処理は簡単なルールに基づいて行われる場合が多い.
一方で, 日本語の文は単語間の区切りが英語ほど明確でないため,
形態素への分割は困難かつ重要である.
本研究では，形態素解析器として Juman++\cite{jumanpp} を用いた．

\changeindent{0cm}
\subsection{分散表現獲得手法}
\changeindent{2cm}
自然言語の単語や文を計算機上で表現するための分散表現獲得手法について説明する.

\changeindent{0cm}
\subsubsection{局所表現}
\changeindent{2cm}
自然言語の単語を計算機上で表現する手法として, 最もシンプルなものが局所表現である.
単語の代表的な局所表現の 1 つに One-hot 表現がある.
One-hot 表現は単語をベクトルの各次元に 1 対 1 対応させる表現方法である.
非常に単純な手法であり, 実装が容易であるという利点がある.
一方で, One-hot 表現では語彙数とベクトルの次元数が等しくなるため,
語彙数の増大とともにベクトルの次元数も増大し, ベクトル空間がスパースになってしまう問題がある.
また, 各単語がベクトル空間上で等距離に配置されてしまうため,
単語間の意味的な関係性については定義できないことも大きな問題である.

\changeindent{0cm}
\subsubsection{分散表現}
\changeindent{2cm}
局所表現の問題点を解決するために考案された手法が分散表現である. 分散表現は各概念をベクトルの単一次元ではなく複数次元の実数で表す.
単語の分散表現は，類似した文脈で使用される単語は類似した意味をもつ，という分布仮説を基盤としている.
単語を実数値密ベクトルで表現することにより,
単語間の意味的な関係性をベクトル空間上での類似度として定義できるという大きな利点がある. 

%\changeindent{0cm}
%\subsubsection{事前学習およびファインチューニング}
%\changeindent{2cm}
%\changeindent{0cm}
%\subsubsection{Attension}
%\changeindent{2cm}
%\changeindent{0cm}
%\subsubsection{Transformers}
%\changeindent{2cm}
%%%%%%%%%%%%%%%%%%
\changeindent{0cm}
\subsubsection{Bidirectional Encoder Representations from Transformers}
\changeindent{2cm}
Bidirectional Encoder Representations from Transformers (BERT) \cite{devlin2018bert} は, 2018 年に
Google が発表した言語モデルであり, 複数の双方向 Transformer に基づく汎用言語モデルである.
これまでの言語モデルは特定の学習タスクに対して 1 つのモデルを用いてきたが, BERT は
大規模コーパスに対して事前学習を施して, 各タスクに対してファインチューニングをすることで,
さまざまなタスクに柔軟に対応することができる.
事前学習には入力の一部の単語を “[MASK]” に置き換えてその元単語を予測するように
訓練するタスクと 2 文を入力としてその連続性を識別するように訓練するタスクが用いられる.
本稿では, 京都大学から公開されている, 日本語 Wikipedia より全 1800 万文を用いて
事前学習されたモデルを使用した.
BERT に文章を入力する際には, 文章の先頭の先頭に “[CLS]” トークンを付与する.
BERT は単語ごとの分散表現を出力するが, “[CLS]” トークンに対する出力を文章全体の分散表現として
扱うことができる. また, 2 文を扱う際には, 文章の間に “[SEP]” トークンを付与する.
